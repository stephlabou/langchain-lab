{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62687ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langchain langchain-community langchainhub openai chromadb bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e173e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a954a000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d14f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install gpt4all > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525e884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28b355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f2510f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "03450615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required for ChatLiteLLM\n",
    "# !pip install google-generativeai \n",
    "# pip install litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa402644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain import hub\n",
    "from langchain_core.messages import AIMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d54a806",
   "metadata": {},
   "source": [
    "## Setting up HuggingFace Hub\n",
    "https://python.langchain.com/docs/integrations/llms/huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc7d13f",
   "metadata": {},
   "source": [
    "Get a token: https://huggingface.co/docs/api-inference/quicktour#get-your-api-token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5967bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "HUGGINGFACEHUB_API_TOKEN = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4087028",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3532ae29",
   "metadata": {},
   "source": [
    "### Testing with a LLM\n",
    "To make sure that the HF API works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aa9b70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who won the FIFA World Cup in the year 1994? \"\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03a5735a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], template=\"Question: {question}\\n\\nAnswer: Let's think step by step.\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f1fc342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xykong/.local/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 1994 FIFA World Cup was held in France. France won the 1994 FIFA World Cup. The answer: France.\n"
     ]
    }
   ],
   "source": [
    "repo_id = \"google/flan-t5-xxl\"\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id, model_kwargs={\"temperature\": 0.1, \"max_length\": 64}\n",
    ")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f08f92",
   "metadata": {},
   "source": [
    "## Setting up for Q&A\n",
    "https://python.langchain.com/docs/use_cases/question_answering/local_retrieval_qa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338fb07d",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9608d461",
   "metadata": {},
   "source": [
    "USSD GIS Data page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "698ff444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"s-lib-box-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://ucsd.libguides.com/gis/gisdata\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e381edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5321"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[0].page_content) # number of str char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d41c290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809f1279",
   "metadata": {},
   "source": [
    "Now we have the GIS data page saved as `docs`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b135233e",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103ccec2",
   "metadata": {},
   "source": [
    "We’ll split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8a7dbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "324f93b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits) # total number of splitted doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bba91c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76db5e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"UC San Diego affiliate can browse the Library's geospatial data holdings that are hosted on the X drive in our Data & GIS Lab. You must be connected to UCSD VPN in order to browse. This is for BROWSING ONLY. You CANNOT DOWNLOAD any files. You must visit the Lab to access files until we are able to publish the data into an online portal (currently in development)\\nThe data is organized geographically, roughly by continent, with topical data arrangement inside each folder.\\nRemember, if you are looking for data on a smaller area of geography, be sure to check the folder with the larger geography first.\\xa0 An example would be if you are looking for data for only one of the states in the United States, you would want to look in the United States Data folder as well as the North America data folder.\", metadata={'source': 'https://ucsd.libguides.com/gis/gisdata', 'start_index': 42})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d6ea65",
   "metadata": {},
   "source": [
    "### Store\n",
    "https://python.langchain.com/docs/integrations/text_embedding/huggingfacehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fa6df68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "\n",
    "embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=HUGGINGFACEHUB_API_TOKEN, model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n",
    ")\n",
    "# vectorstore = Chroma.from_documents(documents=all_splits, embedding=embeddings)\n",
    "vectorstore = FAISS.from_documents(documents=all_splits, embedding=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9da9d6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x7f93a66f41c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fb383b",
   "metadata": {},
   "source": [
    "### Retrieve relevant documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ab17d6",
   "metadata": {},
   "source": [
    "Options:\n",
    "1. Chroma (example from LangChain)\n",
    "2. FAISS (Facebook AI Similarity Search) - using this because it has score function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbaccaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve relevant documents given a query, i.e. question\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b593fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is X drive?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c016ccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8122561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_docs) # should have the same number as `k` specified above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ff81741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UC San Diego affiliate can browse the Library's geospatial data holdings that are hosted on the X drive in our Data & GIS Lab. You must be connected to UCSD VPN in order to browse. This is for BROWSING ONLY. You CANNOT DOWNLOAD any files. You must visit the Lab to access files until we are able to publish the data into an online portal (currently in development)\n",
      "The data is organized geographically, roughly by continent, with topical data arrangement inside each folder.\n",
      "Remember, if you are looking for data on a smaller area of geography, be sure to check the folder with the larger geography first.  An example would be if you are looking for data for only one of the states in the United States, you would want to look in the United States Data folder as well as the North America data folder.\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106a65fa",
   "metadata": {},
   "source": [
    "#### To view scores\n",
    "\n",
    "Note that **the returned distance score is L2 distance. Therefore, a lower score is better.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01270711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='Browse the GIS data holdings on X drive', metadata={'source': 'https://ucsd.libguides.com/gis/gisdata', 'start_index': 1}),\n",
       "  0.84911835),\n",
       " (Document(page_content=\"UC San Diego affiliate can browse the Library's geospatial data holdings that are hosted on the X drive in our Data & GIS Lab. You must be connected to UCSD VPN in order to browse. This is for BROWSING ONLY. You CANNOT DOWNLOAD any files. You must visit the Lab to access files until we are able to publish the data into an online portal (currently in development)\\nThe data is organized geographically, roughly by continent, with topical data arrangement inside each folder.\\nRemember, if you are looking for data on a smaller area of geography, be sure to check the folder with the larger geography first.\\xa0 An example would be if you are looking for data for only one of the states in the United States, you would want to look in the United States Data folder as well as the North America data folder.\", metadata={'source': 'https://ucsd.libguides.com/gis/gisdata', 'start_index': 42}),\n",
       "  1.4540023),\n",
       " (Document(page_content='data for San Diego. Almost 100 free GIS data layers available for public download. Majority of these, including historical dates, are available through the Geisel Library Data & GIS Lab with improved metadata.SimplyAnalytics SimplyAnalytics is a web-based mapping application that lets users quickly create professional-quality thematic maps and reports using demographic, business, and marketing data for the United States. Access limited to 5 simultaneous users. Try again later if refused. Includes EASI/MediaMark Research Incorporated (MRI) and Simmons LOCAL data packages. (formerly known as SimplyMap).UNEP - World Environment Situation Room (WESR)The World Environment Situation Room (WESR) is the new UNEP data platform that provides federated environmental data access at the global, regional, national and local levels for sustainable development and national planning needs. The WESR data system brings together over 45 data platforms and 70 data sources.Contact GIS LibrarianIf you find', metadata={'source': 'https://ucsd.libguides.com/gis/gisdata', 'start_index': 4173}),\n",
       "  1.6656462),\n",
       " (Document(page_content='California State GeoportalThe California State Geoportal is a centralized geographic open data portal, which includes authoritative data and applications from a multitude of California state entities. These State entities cover many topics including Water, Health, Energy, Environment, Transportation, Land Use, Boundaries, Geology, Wildlife, Education, Economy and Fire.CIESIN & SEDAC & WDC (from Columbia University)A very rich website with lots of GIS data from Columbia University portals such as CIESIN (Center for International Earth Science Information Network), SEDAC (Socioeconomic Data and Applications Center), and the World Data Center for Human Interactions.Data is PluralData is Plural is a weekly newsletter of useful/curious datasets, published by Jeremy Singer-Vine. You can browse the full data listings in a Google Spreadsheet of random datasets. A great find!DIVA GIS - DataA comprehensive and great starting point for files on administrative boundaries, water, roads, railroads,', metadata={'source': 'https://ucsd.libguides.com/gis/gisdata', 'start_index': 1794}),\n",
       "  1.6741705)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search_with_score(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6982659",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769cecb5",
   "metadata": {},
   "source": [
    "#### Using HuggingFace LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776ab851",
   "metadata": {},
   "source": [
    "Note: \n",
    "- Use [`HuggingFacePipeline`](https://python.langchain.com/docs/integrations/llms/huggingface_pipelines) to use [LCEL chain](https://python.langchain.com/docs/expression_language/why).\n",
    "\n",
    "- Using `HuggingFaceHub` requires another way of creating chain, see [here](https://python.langchain.com/docs/integrations/llms/huggingface_hub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e93972c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xykong/.local/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "repo_id = \"google/flan-t5-xxl\"\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 1000}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "410184b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'University of California, San Diego'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is UCSD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "316edc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16c6dd27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"X drive is a drive where UC San Diego affiliate can browse the Library's geospatial data holdings.\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"what is X drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14d8b50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\", \"context\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f84d6ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], template='Use the following pieces of context to answer the question at the end.\\nIf you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\nUse three sentences maximum and keep the answer as concise as possible.\\nAlways say \"thanks for asking!\" at the end of the answer.\\n\\n{context}\\n\\nQuestion: {question}\\n\\nHelpful Answer:')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e87a40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X drive is a hard drive in the computer system that contains the operating system and other software that is required to use the library's computers and network.\n"
     ]
    }
   ],
   "source": [
    "print(llm_chain.run({\"question\": \"What is X drive at UC San Diego Library? Explain what is in it?\", \n",
    "                     \"context\": retriever | format_docs}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5515c7e0",
   "metadata": {},
   "source": [
    "#### Thoughts\n",
    "Notice how it doesn't say \"thanks for asking\" at the end of the answer, this is because the model is meant for question-answering tasks. Feeding in context is not guaranteed to work as well because it largely depends on the data this model was trained on. So we either:\n",
    "1. Train/finetune this question-answering model\n",
    "2. Use another type of LLM model that generate responses from context but not actually \"answering\" - perhaps this is called a chat model?\n",
    "\n",
    "Some more useful information https://stackoverflow.com/questions/76963864/low-score-and-wrong-answer-for-flan-t5-xxl-question-answering-task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f57d96a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceInferenceAPIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f93a66f41c0>, search_kwargs={'k': 2})\n",
       "| RunnableLambda(format_docs)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever | format_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7830d804",
   "metadata": {},
   "source": [
    "#### Using chat models\n",
    "https://python.langchain.com/docs/integrations/chat/litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e42837b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xykong/.local/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "repo_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id, \n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\"temperature\": 0.1, \"max_length\": 1000}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c25684e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "The context is: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\", \"context\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ece22d88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: X drive refers to a network storage device commonly used in organizations to share files among employees. It allows users to access and save files from any computer connected to the network. Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "print(llm_chain.run({\"question\": \"What is X drive?\", \n",
    "                     \"context\": retriever | format_docs}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3516957c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: filler question \\nContext: filler context \\nAnswer:\")]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "example_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d9aa02e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8f2cb56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " X drive is a storage location for UC San Diego's geospatial data holdings, accessible through the Library's Data & GIS Lab. It can only be browsed while connected to UCSD VPN, and downloading is not allowed. The data is organized by continent and topic within each folder. It's recommended to check larger geographic folders for data within smaller areas."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\"What is X drive?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b865022b",
   "metadata": {},
   "source": [
    "Confirmed that Retriever + Chat Model works!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf9f451",
   "metadata": {},
   "source": [
    "## With Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824c6f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "contextualize_q_chain = contextualize_q_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e04aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "contextualize_q_chain = qa_prompt | llm | StrOutputParser()\n",
    "\n",
    "def contextualized_question(input: dict):\n",
    "    if input.get(\"chat_history\"):\n",
    "        return contextualize_q_chain\n",
    "    else:\n",
    "        return input[\"question\"]\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context=contextualized_question | retriever | format_docs\n",
    "    )\n",
    "    | qa_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8612617",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "question = \"What is X drive?\"\n",
    "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg])\n",
    "\n",
    "second_question = \"What can I find in the X drive?\"\n",
    "rag_chain.invoke({\"question\": second_question, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1364de90",
   "metadata": {},
   "source": [
    "### Adding chat history memory\n",
    "https://python.langchain.com/docs/expression_language/how_to/message_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4965681e",
   "metadata": {},
   "source": [
    "For building web app:\n",
    "1. https://www.youtube.com/watch?v=rxOeOD98AEU\n",
    "2. https://www.youtube.com/watch?v=O6BB08Zo2uk\n",
    "3. https://www.youtube.com/watch?v=-XirZSq6Wcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c81a0eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
