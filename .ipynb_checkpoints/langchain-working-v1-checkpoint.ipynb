{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b7065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langchain langchain-community langchainhub bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35f6f9fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import bs4\n",
    "from getpass import getpass\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec51dd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "HUGGINGFACEHUB_API_TOKEN = getpass()\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55b4156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_path = [\"https://ucsd.libguides.com/gis/gisdata\",\n",
    "    #       \"https://ucsd.libguides.com/az.php\"\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "077380af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HUGGINGFACEHUB_API_TOKEN': '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',\n",
       " 'emb_model_name': 'sentence-transformers/all-MiniLM-l6-v2',\n",
       " 'llm_repo_id': 'HuggingFaceH4/zephyr-7b-beta',\n",
       " 'llm_task': 'text-generation',\n",
       " 'response_max_length': 1000}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = util.load_config('config/config.yaml')\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5269113b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sentence-transformers/all-MiniLM-l6-v2'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4990d0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_webpage(web_path, use_strainer=False, strainer_class=None):\n",
    "    loader = None\n",
    "    if use_strainer:\n",
    "        bs4_strainer = bs4.SoupStrainer(class_=(strainer_class))\n",
    "        loader = WebBaseLoader(\n",
    "            web_paths=(web_path),\n",
    "            bs_kwargs={\"parse_only\": bs4_strainer},\n",
    "        )\n",
    "    else: \n",
    "        loader = WebBaseLoader(web_paths=(web_path))\n",
    "    docs = loader.load()\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3357295",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_webpage([\"https://ucsd.libguides.com/gis/gisdata\"], use_strainer=True, strainer_class='s-lib-box-content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96e3a919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(docs, chunk_size=1000, chunk_overlap=200, add_start_index=True):\n",
    "    # split document\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, add_start_index=add_start_index\n",
    "    )\n",
    "    return text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ad97e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs = split_documents(docs)\n",
    "embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
    "        api_key=config['HUGGINGFACEHUB_API_TOKEN'], model_name=config['emb_model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc441a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retriever(config, embeddings, documents, search_type=\"similarity\", topk=2):\n",
    "    vectorstore = FAISS.from_documents(documents=documents, embedding=embeddings)\n",
    "    \n",
    "    retriever = vectorstore.as_retriever(search_type=search_type, search_kwargs={\"k\": topk})\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "701fb76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceInferenceAPIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f311e0fab20>, search_kwargs={'k': 2})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store as a vector\n",
    "get_retriever(config, embeddings, split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "919e3a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm(config):\n",
    "    llm = HuggingFaceHub(\n",
    "        repo_id=config['llm_repo_id'], \n",
    "        task=config['llm_task'],\n",
    "        model_kwargs={\"temperature\": 0.1, \"max_length\": config['response_max_length']}\n",
    "    )\n",
    "    \n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8d5aa92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xykong/.local/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "llm = create_llm(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "021163a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_prompt():\n",
    "    template = \"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question.\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    Use three sentences maximum and keep the answer as concise as possible.\n",
    "    Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Helpful answer:\"\"\"\n",
    "\n",
    "    custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "    \n",
    "    return custom_rag_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a83e016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9e55d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c345a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], template='You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question.\\n    If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n    Use three sentences maximum and keep the answer as concise as possible.\\n    Always say \"thanks for asking!\" at the end of the answer.\\n\\n    {context}\\n\\n    Question: {question}\\n\\n    Helpful answer:')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_custom_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc1978b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is X drive?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c53d0d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18fd34ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The X drive is a storage location where UC San Diego affiliates can browse the Library's geospatial data holdings. However, you cannot download any files from there. You must be connected to UCSD VPN and visit the Data & GIS Lab to access files until an online portal is developed. Thanks for asking!\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "91e13b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG():\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.llm = create_llm(config)\n",
    "        self.embeddings = create_embeddings(config)\n",
    "        self.prompt = create_custom_prompt()\n",
    "        self.docs = None\n",
    "        self.split_docs = None\n",
    "        self.retriever = None\n",
    "        self.called = False\n",
    "        \n",
    "    def __call__(self, web_path, use_strainer=False, strainer_class=None, split=True, chunk_size=1000, chunk_overlap=200, add_start_index=True, search_type=\"similarity\", topk=2):\n",
    "        load_webpage(web_path, use_strainer, strainer_class)\n",
    "        split_documents(chunk_size, chunk_overlap, add_start_index)\n",
    "        get_retriever()\n",
    "        self.custom_rag_chain = (\n",
    "                                    {\"context\": self.retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "                                    | self.prompt\n",
    "                                    | self.llm\n",
    "                                    | StrOutputParser()\n",
    "                                )\n",
    "    def ask(self, question):\n",
    "        if not self.called:\n",
    "            assert False, \"RAG not initialized with context. Please provide web paths\"\n",
    "        return self.custom_rag_chain.invoke(question)\n",
    "    \n",
    "    def load_webpage(self, web_path, use_strainer, strainer_class):\n",
    "        loader = None\n",
    "        if use_strainer:\n",
    "            bs4_strainer = bs4.SoupStrainer(class_=(strainer_class))\n",
    "            loader = WebBaseLoader(\n",
    "                web_paths=(web_path),\n",
    "                bs_kwargs={\"parse_only\": bs4_strainer},\n",
    "            )\n",
    "        else: \n",
    "            loader = WebBaseLoader(web_paths=(web_path))\n",
    "        self.docs = loader.load()\n",
    "    \n",
    "    def split_documents(self, chunk_size, chunk_overlap, add_start_index):\n",
    "        # split document\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size, chunk_overlap=chunk_overlap, add_start_index=add_start_index\n",
    "        )\n",
    "        self.split_docs = text_splitter.split_documents(self.docs)\n",
    "    \n",
    "    def get_retriever(self, search_type, topk):\n",
    "        vectorstore = FAISS.from_documents(documents=self.split_docs, embedding=self.embeddings)\n",
    "        retriever = vectorstore.as_retriever(search_type=search_type, search_kwargs={\"k\": topk})\n",
    "        self.retriever = retriever\n",
    "\n",
    "    def create_llm(self, config):\n",
    "        llm = HuggingFaceHub(\n",
    "            repo_id=config['llm_repo_id'], \n",
    "            task=config['llm_task'],\n",
    "            model_kwargs={\"temperature\": 0.1, \"max_length\": config['response_max_length']}\n",
    "        )\n",
    "\n",
    "        return llm\n",
    "    \n",
    "    def create_embeddings(self, config):\n",
    "        return HuggingFaceInferenceAPIEmbeddings(api_key=config['HUGGINGFACEHUB_API_TOKEN'], model_name=config['emb_model_name'])\n",
    "    \n",
    "    def create_custom_prompt(self):\n",
    "        template = \"\"\"You are an assistant for question-answering tasks. \n",
    "        Use the following pieces of retrieved context to answer the question.\n",
    "        If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "        Use three sentences maximum and keep the answer as concise as possible.\n",
    "        Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Helpful answer:\"\"\"\n",
    "\n",
    "        custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "        return custom_rag_prompt\n",
    "    \n",
    "    def format_docs(self, docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "920fbf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xykong/.local/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'create_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_124/2724896133.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRAG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_124/677286382.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_custom_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "rag = RAG(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fdc002",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-venv",
   "language": "python",
   "name": "langchain-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
